{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4639999f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import pos_tag\n",
    "from nltk import FreqDist\n",
    "\n",
    "# Ensure necessary NLTK resources are downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Example raw dataset loading\n",
    "train = pd.read_csv('train.csv')\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    \n",
    "    # Remove non-alphanumeric characters (punctuation, etc.)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply text cleaning to the raw dataset\n",
    "train['text'] = train['text'].apply(clean_text)\n",
    "\n",
    "# Function to calculate positive and negative scores (simple example using lexicons)\n",
    "def sentiment_scores(text):\n",
    "    positive_words = ['good', 'great', 'awesome', 'positive', 'happy']\n",
    "    negative_words = ['bad', 'sad', 'angry', 'negative', 'hate']\n",
    "    \n",
    "    positive_score = sum(1 for word in text.split() if word in positive_words)\n",
    "    negative_score = sum(1 for word in text.split() if word in negative_words)\n",
    "    \n",
    "    return positive_score, negative_score\n",
    "\n",
    "# Apply sentiment score calculation\n",
    "train['positive_score'], train['negative_score'] = zip(*train['text'].apply(sentiment_scores))\n",
    "\n",
    "# Calculate text length\n",
    "train['text_length'] = train['text'].apply(len)\n",
    "\n",
    "# Entity count (example)\n",
    "# Let's count words with proper nouns (simplified entity extraction)\n",
    "def entity_count(text):\n",
    "    tagged = pos_tag(word_tokenize(text))\n",
    "    return sum(1 for word, tag in tagged if tag == 'NNP')  # NNP -> Proper Noun\n",
    "\n",
    "train['entity_count'] = train['text'].apply(entity_count)\n",
    "\n",
    "# Part of Speech Ratios (Adjective, Verb, Noun)\n",
    "def pos_ratios(text):\n",
    "    tagged = pos_tag(word_tokenize(text))\n",
    "    adj = sum(1 for word, tag in tagged if tag == 'JJ')  # Adjective\n",
    "    verb = sum(1 for word, tag in tagged if tag.startswith('VB'))  # Verb\n",
    "    noun = sum(1 for word, tag in tagged if tag.startswith('NN'))  # Noun\n",
    "    \n",
    "    total_tokens = len(tagged)\n",
    "    adj_ratio = adj / total_tokens if total_tokens > 0 else 0\n",
    "    verb_ratio = verb / total_tokens if total_tokens > 0 else 0\n",
    "    noun_ratio = noun / total_tokens if total_tokens > 0 else 0\n",
    "    \n",
    "    return adj_ratio, verb_ratio, noun_ratio\n",
    "\n",
    "train['adj_ratio'], train['verb_ratio'], train['noun_ratio'] = zip(*train['text'].apply(pos_ratios))\n",
    "\n",
    "# Negation flag (simple check for negations like 'not', 'never')\n",
    "def negation_flag(text):\n",
    "    negations = ['not', 'never', 'no']\n",
    "    return 1 if any(neg in text for neg in negations) else 0\n",
    "\n",
    "train['negation_flag'] = train['text'].apply(negation_flag)\n",
    "\n",
    "# Final preprocessed data\n",
    "df = train[['text', 'sentiment', 'text_length', 'positive_score', 'negative_score', 'entity_count', 'adj_ratio', 'verb_ratio', 'noun_ratio', 'negation_flag']]\n",
    "\n",
    "# Show the preprocessed data\n",
    "print(df.head())\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
